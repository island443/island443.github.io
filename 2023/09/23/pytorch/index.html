<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.17.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="pytorch官方文档：PyTorch">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch">
<meta property="og:url" content="http://example.com/2023/09/23/pytorch/index.html">
<meta property="og:site_name" content="island443">
<meta property="og:description" content="pytorch官方文档：PyTorch">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic.imgdb.cn/item/65003de4661c6c8e54f1a98a.png">
<meta property="og:image" content="https://pic.imgdb.cn/item/65004e76661c6c8e54fa7ab3.png">
<meta property="og:image" content="https://pic.imgdb.cn/item/65005bc2661c6c8e5402e000.png">
<meta property="og:image" content="https://pic.imgdb.cn/item/6501b593661c6c8e54945c31.png">
<meta property="og:image" content="https://pic.imgdb.cn/item/6501e5a8661c6c8e54a91cbc.png">
<meta property="og:image" content="https://pic.imgdb.cn/item/6502dff8661c6c8e54fe608b.png">
<meta property="og:image" content="https://z1.ax1x.com/2023/09/16/pPfa5DK.png">
<meta property="og:image" content="https://z1.ax1x.com/2023/09/16/pPfaIHO.png">
<meta property="og:image" content="https://z1.ax1x.com/2023/09/16/pPfaTED.png">
<meta property="og:image" content="https://pic.imgdb.cn/item/6502fe51661c6c8e540da5e6.png">
<meta property="og:image" content="https://z1.ax1x.com/2023/09/16/pPfaH4H.png">
<meta property="og:image" content="https://z1.ax1x.com/2023/09/16/pPfaR3R.png">
<meta property="og:image" content="https://z1.ax1x.com/2023/09/16/pPfwnyt.png">
<meta property="og:image" content="https://z1.ax1x.com/2023/09/16/pPf05D0.png">
<meta property="og:image" content="https://z1.ax1x.com/2023/09/16/pPfr6UK.png">
<meta property="og:image" content="https://z1.ax1x.com/2023/09/16/pPfszSe.png">
<meta property="og:image" content="https://z1.ax1x.com/2023/09/16/pPfgOnH.png">
<meta property="og:image" content="https://z1.ax1x.com/2023/09/17/pPfzYCV.png">
<meta property="og:image" content="https://z1.ax1x.com/2023/09/17/pPhS1MD.png">
<meta property="og:image" content="https://z1.ax1x.com/2023/09/17/pPhStII.png">
<meta property="og:image" content="https://z1.ax1x.com/2023/09/17/pPhSdRf.png">
<meta property="og:image" content="https://z1.ax1x.com/2023/09/17/pPhSHoR.png">
<meta property="og:image" content="https://z1.ax1x.com/2023/09/17/pPhSqF1.png">
<meta property="og:image" content="https://z1.ax1x.com/2023/09/17/pPhSOW6.png">
<meta property="article:published_time" content="2023-09-23T00:55:01.000Z">
<meta property="article:modified_time" content="2023-09-23T01:09:43.989Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic.imgdb.cn/item/65003de4661c6c8e54f1a98a.png">


<link rel="canonical" href="http://example.com/2023/09/23/pytorch/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2023/09/23/pytorch/","path":"2023/09/23/pytorch/","title":"pytorch"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>pytorch | island443</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">island443</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#pytorch"><span class="nav-number">1.</span> <span class="nav-text">pytorch</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">1.1.</span> <span class="nav-text">1.数据预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%881%EF%BC%89%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="nav-number">1.1.1.</span> <span class="nav-text">（1）数据加载</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%882%EF%BC%89%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-number">1.1.2.</span> <span class="nav-text">（2）可视化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%883%EF%BC%89transformer"><span class="nav-number">1.1.3.</span> <span class="nav-text">（3）transformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A5%E4%B8%8A%E7%9F%A5%E8%AF%86%E7%82%B9%E8%81%94%E5%90%88"><span class="nav-number">1.1.4.</span> <span class="nav-text">以上知识点联合</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CNeural-Network"><span class="nav-number">1.2.</span> <span class="nav-text">2.搭建神经网络Neural Network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%881%EF%BC%89%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">1.2.1.</span> <span class="nav-text">（1）卷积层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%882%EF%BC%89%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-number">1.2.2.</span> <span class="nav-text">（2）最大池化层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E7%BA%BF%E6%80%A7%E5%B1%82%EF%BC%88%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%EF%BC%89"><span class="nav-number">1.2.3.</span> <span class="nav-text">(3)线性层（全连接层）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%884%EF%BC%89CIFAR10%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.2.4.</span> <span class="nav-text">（4）CIFAR10模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-%EF%BC%8C%E4%BC%98%E5%8C%96%E5%99%A8"><span class="nav-number">1.3.</span> <span class="nav-text">损失函数，反向传播 ，优化器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%93%8D%E4%BD%9C"><span class="nav-number">1.4.</span> <span class="nav-text">模型操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83%E5%A5%97%E8%B7%AF"><span class="nav-number">1.5.</span> <span class="nav-text">模型的训练套路</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A1%A5%E5%85%85%E9%81%87%E5%88%B0%E7%9A%84%E5%87%BD%E6%95%B0"><span class="nav-number">1.6.</span> <span class="nav-text">补充遇到的函数</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt=""
      src="/image%5Cavatar.jpg">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description">非志无以成学</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">20</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/island443" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;island443" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:2301062786@qq.com" title="E-Mail → mailto:2301062786@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/09/23/pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/image%5Cavatar.jpg">
      <meta itemprop="name" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="island443">
      <meta itemprop="description" content="非志无以成学">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="pytorch | island443">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          pytorch
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2023-09-23 08:55:01 / 修改时间：09:09:43" itemprop="dateCreated datePublished" datetime="2023-09-23T08:55:01+08:00">2023-09-23</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h1><p>官方文档：<a target="_blank" rel="noopener" href="https://pytorch.org/">PyTorch</a></p>
<span id="more"></span>

<p><strong>argparse是python标准库，用于解析命令行参数和选项</strong>，程序定义它需要的参数，然后argparse会从sys.argv解析出这些参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 创建解析器对象  </span></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;这是一个示例程序&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 添加参数  </span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;integers&#x27;</span>, metavar=<span class="string">&#x27;N&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, nargs=<span class="string">&#x27;+&#x27;</span>,  </span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;整数列表&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line">parser.add_argument(<span class="string">&#x27;--sum&#x27;</span>, dest=<span class="string">&#x27;compute_sum&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>,  </span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;求和输入的整数（默认：找出最大值）&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 解析参数  </span></span><br><span class="line">args = parser.parse_args()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 使用参数  </span></span><br><span class="line"><span class="keyword">if</span> args.compute_sum:  </span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">sum</span>(args.integers))  </span><br><span class="line"><span class="keyword">else</span>:  </span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">max</span>(args.integers))</span><br></pre></td></tr></table></figure>







<h2 id="1-数据预处理"><a href="#1-数据预处理" class="headerlink" title="1.数据预处理"></a>1.数据预处理</h2><h3 id="（1）数据加载"><a href="#（1）数据加载" class="headerlink" title="（1）数据加载"></a>（1）数据加载</h3><p><strong>Dataset 和Dataloader</strong></p>
<p>Dataset提供一种方式去获取每一个数据及其label（<strong>告诉计算机数据在什么地方，以及第一个数据第二个数据是什么的问题</strong>）</p>
<p>一种常见的数据集的存储方式，train训练数据集下，分别有ants和bees的文件夹，各自文件夹内有其各自的图片数据，而文件夹名则为文件夹内部数据的label标签</p>
<p><img src="https://pic.imgdb.cn/item/65003de4661c6c8e54f1a98a.png"></p>
<p>Dataset是一个抽象类，代表一个数据集。你可以使用Dataset类去创建一个属于你的数据集。</p>
<p>使用时必须创建一个类，让这个类继承Dataset类，重写<code>__len__</code>和<code>__getitem__</code>方法。<code>__len__</code>应该返回数据集的总大小，而<code>__getitem__</code>则应根据索引返回一个样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,root_dir,label_dir</span>):</span><br><span class="line">        self.root_dir=root_dir                      <span class="comment"># 训练数据集地址</span></span><br><span class="line">        self.label_dir=label_dir                    <span class="comment"># 标签名</span></span><br><span class="line">        self.path=os.path.join(self.root_dir,self.label_dir)    <span class="comment"># 利用os的函数把地址连接起来，方便在不同环境使用</span></span><br><span class="line">        self.img_path=os.listdir(self.path)         <span class="comment"># os.listdir函数返回self.path路径下的所以文件和文件夹</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img_name=self.img_path[idx]</span><br><span class="line">        img_item_path=os.path.join(self.root_dir,self.label_dir,img_name)</span><br><span class="line">        img=Image.<span class="built_in">open</span>(img_item_path)               <span class="comment"># 打开图片，以后可以对图片进行各种操作</span></span><br><span class="line">        label=self.label_dir</span><br><span class="line">        <span class="keyword">return</span> img,label                            <span class="comment"># 返回是图片和标签</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root_dir=<span class="string">&quot;dataset/train&quot;</span></span><br><span class="line">ants_label_dir=<span class="string">&quot;ants&quot;</span></span><br><span class="line">bees_label_dir=<span class="string">&quot;bees&quot;</span></span><br><span class="line"></span><br><span class="line">ants_dataset=MyDataset(root_dir,ants_label_dir)</span><br><span class="line">bees_dataset=MyDataset(root_dir,bees_label_dir)</span><br><span class="line"></span><br><span class="line">train_dataset= ants_dataset+bees_dataset</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(bees_dataset.__len__())</span><br><span class="line">bees_dataset.__getitem__(<span class="number">1</span>)[<span class="number">0</span>].show()   </span><br></pre></td></tr></table></figure>



<p>当然，还有另一种数据的存储方式，图片在image下的文件夹，标签在label文件夹下同名的txt记事本里</p>
<p><img src="https://pic.imgdb.cn/item/65004e76661c6c8e54fa7ab3.png"></p>
<p>Dataloader为后面的网络提供不同的数据形式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">test_dataset=torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset2&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个参数表明是哪个数据集 batch_size表明抓几个数据打包 shuffle表示下一次是否打乱数据 drop_last最后一个batch表示不足是否去掉</span></span><br><span class="line">test_dataloader=DataLoader(test_dataset,batch_size=<span class="number">4</span>,shuffle=<span class="literal">True</span>,drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">    imgs , targets = data</span><br></pre></td></tr></table></figure>







<h3 id="（2）可视化"><a href="#（2）可视化" class="headerlink" title="（2）可视化"></a>（2）可视化</h3><p>在PyTorch中，可以使用TensorBoard来可视化模型训练过程。</p>
<p>使用<code>writer.add_scalar()</code>函数来记录标量值（例如损失和准确性）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer=SummaryWriter(<span class="string">&quot;log&quot;</span>)          <span class="comment"># 参数log的意思的把记录存在log文件夹下</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;y=x&quot;</span>,i,i)     <span class="comment"># 参数分别为 tag函数名称  value函数值  step横坐标步数</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p>在控制台输入 tensorboard –logdir&#x3D;log可以查看可视化图表</p>
<h3 id="（3）transformer"><a href="#（3）transformer" class="headerlink" title="（3）transformer"></a>（3）transformer</h3><img src="https://pic.imgdb.cn/item/65005bc2661c6c8e5402e000.png" style="zoom: 25%;" />



<p>Tensor类型数据是<strong>一种能够容纳任意数据类型的多维数组</strong>，也被广泛应用于深度学习和机器学习等领域。</p>
<p>Tensor可以理解为张量，在维度数方面与矩阵类似，但在元素数量方面却没有限制。它可以是一个标量、一个向量，也可以是一个矩阵，甚至是更高维度的数据结构。它能够表达各种各样的数据类型，比如文本、图像、声音等等。</p>
<p><strong>Tosor（）和Normalize（）</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">img_path=<span class="string">&quot;dataset/train/ants/0013035.jpg&quot;</span></span><br><span class="line">img=Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(img))                <span class="comment"># &lt;class &#x27;PIL.JpegImagePlugin.JpegImageFile&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ToTensor</span></span><br><span class="line">tensor_tr =transforms.ToTensor()	<span class="comment"># ToTensor()，它用于将PIL Image或者NumPy的ndarray转换为PyTorch的Tensor，</span></span><br><span class="line">								    <span class="comment"># 并且把像素的强度值从0-255变换到0-1之间   （opencv的图片格式是ndarray格式）</span></span><br><span class="line">tensor_img=tensor_tr(img)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(tensor_img))         <span class="comment"># &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line">write=SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">write.add_image(<span class="string">&quot;img_tag&quot;</span>,tensor_img)  <span class="comment"># 第一个参数是图片标签，第二个参数是tensor或ndarray类型的图片，第三参数是step</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize</span></span><br><span class="line">trans_norm=transforms.Normalize([<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>],[<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>]) <span class="comment"># 参数为mean和std，分别代表RGB三通道的均值和标准差，归															   # 一化后的值 = (原值 - 均值) / 标准差</span></span><br><span class="line">img_norm=trans_norm(tensor_img)          <span class="comment"># tensor_是一个tensor对象，并且是[0,1]的浮点数，经过归一化后范围变为[-1，1]</span></span><br><span class="line">write.add_image(<span class="string">&quot;img_normalize&quot;</span>,img_norm)          </span><br><span class="line"><span class="comment">#在Transformer模型中，归一化被用于对输入序列进行预处理，使得不同位置的词向量具有相似的尺度，从而提高模型的性能</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">write.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在这段代码中，<code>tensor_tr = transforms.ToTensor()</code>是创建一个ToTensor的实例，然后<code>tensor_img = tensor_tr(img)</code>是将img转换为Tensor。</p>
<p>在归一化函数中，为什么均差和标准值都是三维度向量？</p>
<p>这是因为我们在处理的是RGB图像，这种图像有三个通道：红色、绿色和蓝色。每个通道都需要单独进行归一化处理，所以需要三个参数。</p>
<h3 id="以上知识点联合"><a href="#以上知识点联合" class="headerlink" title="以上知识点联合"></a>以上知识点联合</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compose用于组合多个图像转换操作，接受一个包含转换操作的列表作为参数，这些操作会按照列表中的顺序进行。返回值是一个可调用对象。</span></span><br><span class="line">dataset_transforms=torchvision.transforms.Compose([torchvision.transforms.ToTensor()]) </span><br><span class="line"><span class="comment"># 切记，ToTensor后的（）不要再次忘记</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数：root数据集存放的位置 train为True为训练集，为False为测试集 transform是要进行什么预处理 download是指是否下载数据集</span></span><br><span class="line">dataset_train = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset2&quot;</span>,train=<span class="literal">True</span>,transform=dataset_transforms,download=<span class="literal">True</span>)</span><br><span class="line">dataset_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset2&quot;</span>,train=<span class="literal">False</span>,transform=dataset_transforms,download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">write=SummaryWriter(<span class="string">&quot;p10&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    img, target= dataset_set[i]       <span class="comment"># 数据集返回一个元组，第一个是tensor形式的图片，一个是图片标签</span></span><br><span class="line">    write.add_image(<span class="string">&quot;img_set&quot;</span>,img,i)</span><br><span class="line"></span><br><span class="line">write.close()</span><br></pre></td></tr></table></figure>











<h2 id="2-搭建神经网络Neural-Network"><a href="#2-搭建神经网络Neural-Network" class="headerlink" title="2.搭建神经网络Neural Network"></a>2.搭建神经网络Neural Network</h2><p>Torch.nn模块是PyTorch中用于构建神经网络的部分</p>
<img src="https://pic.imgdb.cn/item/6501b593661c6c8e54945c31.png" style="zoom: 50%;" />

<p><strong>torch.nn.Module是一个基类，用于创建、操作和管理神经网络模型</strong></p>
<p>如果想创建自己的模型，应该继承nn.Module并重载其—init—、forward和其他额外函数。当实例化你的模型时，只要传入对应的参数，就会自动调用forward函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module1</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):			<span class="comment"># 初始化函数</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()		<span class="comment"># 加载父类的初始化函数</span></span><br><span class="line">        						<span class="comment"># 有关self的赋值语句写在这里,不管是新建变量，还是某些torch.nn中的函数返回的对象</span></span><br><span class="line">	</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):	<span class="comment">#前向传播函数，在神经网络中，前向传播函数定义了输入数据的处理流程</span></span><br><span class="line">        output = <span class="built_in">input</span> + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">m1 = Module1()</span><br><span class="line">x=torch.tensor(<span class="number">1.0</span>)     <span class="comment">#创建一个PyTorch的tensor对象x，数值为1.0</span></span><br><span class="line">output=m1(x)			<span class="comment">#调用m1对象的前向传播函数,输入是x，返回值赋值给output</span></span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>

<p>在PyTorch中，当像调用一个函数一样调用一个模型对象（如 m1(x)）时，实际上是在进行前向传播。模型对象中的__call__方法被设计成执行前向传播，这就是为什么可以通过直接调用模型对象来进行前向传播的原因。</p>
<p>在这个例子中，m1(x)实际上等价于m1.forward(x)，两者都会触发前向传播。m1(x)只是一种更简洁的写法。</p>
<p><strong>卷积操作</strong></p>
<p>卷积的计算方式：把卷积和放到左上角，然后9个数相乘相加后即为第一个数，然后按照stride规定的步长移动，计算其他值</p>
<p><img src="https://pic.imgdb.cn/item/6501e5a8661c6c8e54a91cbc.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  torch</span><br><span class="line"><span class="keyword">import</span>  torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">                     [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">                     [<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                     [<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                     [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">kernel = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>],</span><br><span class="line">                     [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">                     [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"> <span class="comment"># 调整变成4个尺寸，以便能进行卷积，一个形状为 (1, 1, 5, 5) 的张量。这表示有 1 个样本，1 个通道，高度为 5，宽度为 5。</span></span><br><span class="line"><span class="built_in">input</span> =  torch.reshape(<span class="built_in">input</span>,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment"># 它是一个形状为 (1, 1, 3, 3) 的张量。这表示有 1 个卷积核，1 个通道，卷积核的高度为 3，宽度为 3。</span></span><br><span class="line">kernel = torch.reshape(kernel,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数 input为输入图像，kernel为卷积核，stride为卷积核移动步长，参数padding可以给输入图像填充行或列</span></span><br><span class="line">output=F.conv2d(<span class="built_in">input</span>,kernel,stride=<span class="number">1</span>) </span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>

<p>卷积在处理图片的作用？</p>
<p><strong>在神经网络中，对图片进行卷积的主要目的是从输入图像中提取有用的特征。</strong></p>
<p>卷积核可以看作是一种滤波器，它定义了一个特定的特征，例如水平或垂直边缘等。卷积操作就是在输入图像上提取这个特征，如果输入图像中的某些像素值符合卷积核特征，那么在输出图像中对应的像素值就会比较大，其他地方的值则会很低。</p>
<p>通过使用不同的卷积核，可以从输入图像中提取不同的特征，在卷积神经网络中，通过使用大量的卷积核，可以提取出图像中的多种特征，并将这些特征组合起来，以做出决策。</p>
<p>另外，卷积还有一些优势，例如权重共享和平移不变性。这些特性使得卷积在处理图像数据时具有高效性和鲁棒性。</p>
<p><strong>神经网络中，参数类型（Size）如果某一维是-1的话，意思是让计算机自己计算这个参数是多少</strong></p>
<h3 id="（1）卷积层"><a href="#（1）卷积层" class="headerlink" title="（1）卷积层"></a>（1）卷积层</h3><p>先来张图，理解理解channel的意思：</p>
<img src="https://pic.imgdb.cn/item/6502dff8661c6c8e54fe608b.png" style="zoom: 33%;" />

<p>in_channel表示输入图像为1个通道，out_channel表示输出图像通道为2，那么就需要两个卷积核。RGB的图形为3通道</p>
<p>在来看conv2d函数的参数，以及输入输出：</p>
<p><strong>Conv2d（in_channels，out_channels，kernel_size，stride，padding，dilation）</strong></p>
<p>参数分别是输入图像通道数（int），输出图像通道数（int），卷积核大小（int或tuple），卷积核步长（int或tuple），填充，空洞卷积</p>
<p>Conv2d 的 forward 方法接收一个输入张量大小为 [batch_size（一批几张图片）, in_channels, height, width]并返回一个输出张量大小为 [batch_size, out_channels, out_height, out_width]，其中输出图像参数是根据输入大小、卷积核大小、步长、填充 计算得出</p>
<p>上代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">datasets = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset2&quot;</span>, transform=torchvision.transforms.ToTensor(), train=<span class="literal">False</span>,</span><br><span class="line">                                        download=<span class="literal">False</span>)</span><br><span class="line">dataloader = DataLoader(datasets, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">C_Network</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 一开始没注意，使用卷积时先定义卷积操作！！！！！</span></span><br><span class="line">        self.conv2 = torch.nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">3</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"> <span class="comment"># 切记神经网络只干神经网络的工作，我为什么老想把数据集进行卷积的循环步骤写在神经网络里呢？？ 菜！ 要理清各部分的工作应该在哪里干</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">c1=C_Network()</span><br><span class="line">write=SummaryWriter(<span class="string">&quot;con2&quot;</span>)</span><br><span class="line">step=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs,targets=data</span><br><span class="line">    conv_imgs=c1(imgs)</span><br><span class="line">    write.add_images(<span class="string">&quot;img&quot;</span>,imgs,step)</span><br><span class="line">    write.add_images(<span class="string">&quot;conv2d&quot;</span>,conv_imgs,step)</span><br><span class="line">    step=step+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">write.close()</span><br></pre></td></tr></table></figure>



<p>神奇，每次卷积处理的图片都不一样</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pPfa5DK"><img src="https://z1.ax1x.com/2023/09/16/pPfa5DK.png" alt="pPfa5DK.png" style="zoom:33%;" /></a></p>
<h3 id="（2）最大池化层"><a href="#（2）最大池化层" class="headerlink" title="（2）最大池化层"></a>（2）最大池化层</h3><p>先上张图，看最大池化是如何计算的：（取池化核范围内最大的一个数）</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pPfaIHO"><img src="https://z1.ax1x.com/2023/09/16/pPfaIHO.png" alt="pPfaIHO.png"></a></p>
<p>池化操作可以减少网络的计算复杂度，点少了好计算，相当于1080p画质降到360p，不影响认知</p>
<p>再开看函数的参数：</p>
<p><strong>Maxpool2d（kernel_size，stride，padding，dilation，ceil_mode）</strong></p>
<p>参数按顺序分别是池化核大小，池化核步长，补足，间隔，元素不够是否保留输出</p>
<p>代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">datasets = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset2&quot;</span>, transform=torchvision.transforms.ToTensor(), train=<span class="literal">False</span>,</span><br><span class="line">                                        download=<span class="literal">False</span>)</span><br><span class="line">dataloader = DataLoader(datasets, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">M_Network</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.maxpool2d=torch.nn.MaxPool2d(kernel_size=<span class="number">3</span>,ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.maxpool2d(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">m1=M_Network()</span><br><span class="line">write=SummaryWriter(<span class="string">&quot;max2&quot;</span>)</span><br><span class="line">step=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs,targets=data</span><br><span class="line">    maxp_imgs=m1(imgs)</span><br><span class="line">    write.add_images(<span class="string">&quot;img&quot;</span>,imgs,step)</span><br><span class="line">    write.add_images(<span class="string">&quot;maxpool&quot;</span>,maxp_imgs,step)</span><br><span class="line">    step=step+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">write.close()</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pPfaTED"><img src="https://z1.ax1x.com/2023/09/16/pPfaTED.png" alt="pPfaTED.png" style="zoom:33%;" /></a><img src="https://pic.imgdb.cn/item/6502fe51661c6c8e540da5e6.png"></p>
<h3 id="3-线性层（全连接层）"><a href="#3-线性层（全连接层）" class="headerlink" title="(3)线性层（全连接层）"></a>(3)线性层（全连接层）</h3><p>在神经网络中，我们通常用线性层完成两层神经元间的     线性变换（y&#x3D;Ax+b）</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pPfaH4H"><img src="https://z1.ax1x.com/2023/09/16/pPfaH4H.png" alt="pPfaH4H.png" style="zoom:33%;" /></a></p>
<p>它对输入数据和权重进行线性变换（k），并可以加上偏置进行偏移（b）</p>
<p>线性层参数 in_features 就是输入x个数，out_features 是线性层的输出是上图的g个数</p>
<p><strong>torch.nn.Linear(in_features，out_features，bias)</strong></p>
<p>参数按顺序分别是：输入特征的数量，即输入层的节点数或输入通道数。输出特征的数量。是否添加偏置项，默认True。</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pPfaR3R"><img src="https://z1.ax1x.com/2023/09/16/pPfaR3R.png" alt="pPfaR3R.png" style="zoom:33%;" /></a></p>
<p>代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn</span><br><span class="line">import torchvision.datasets</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">datasets = torchvision.datasets.CIFAR10(&quot;./dataset2&quot;, transform=torchvision.transforms.ToTensor(), train=False,</span><br><span class="line">                                        download=False)</span><br><span class="line">dataloader = DataLoader(datasets, batch_size=64)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Model(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.linear = nn.Linear(196608, 10)  # 有196608个输入和10个输出</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        return self.linear(input)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">m1 = Model()</span><br><span class="line"></span><br><span class="line">for data in dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    print(imgs.shape)  # [64, 3, 32, 32] 批次大小 通道数 长 宽</span><br><span class="line">    input = torch.reshape(imgs, (1, 1, 1, -1))  # -1表示让计算机自己算这个参数</span><br><span class="line">    print(input.shape)  # [1, 1, 1, 196608]   # 拉伸成向量</span><br><span class="line">    output=m1(input)</span><br><span class="line">    print(output.shape)   # [1, 1, 1, 10] 表示一张图片，一个通道，一行，十列</span><br></pre></td></tr></table></figure>







<h3 id="（4）CIFAR10模型"><a href="#（4）CIFAR10模型" class="headerlink" title="（4）CIFAR10模型"></a>（4）CIFAR10模型</h3><p>从网上查阅得知cifar10的模型为：（最后outputs为10是为了做一个10分类）</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pPfwnyt"><img src="https://z1.ax1x.com/2023/09/16/pPfwnyt.png" alt="pPfwnyt.png"></a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class CIFAR10(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(3, 32, 5, padding=2)</span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(2)</span><br><span class="line">        self.conv2 = nn.Conv2d(32, 32, 5, padding=2)</span><br><span class="line">        self.maxpool2 = nn.MaxPool2d(2)</span><br><span class="line">        self.conv3 = nn.Conv2d(32, 64, 5, padding=2)</span><br><span class="line">        self.maxpool3 = nn.MaxPool2d(2)</span><br><span class="line">        self.flatten = nn.Flatten()  			# 拉平，64通道X3X3=1024</span><br><span class="line">        self.linear1 = nn.Linear(1024, 64)		#  1024--&gt;64</span><br><span class="line">        self.linear2 = nn.Linear(64, 10)		#  64--&gt;10</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.conv1(x)  # 不能加逗号可能导致报错</span><br><span class="line">        x = self.maxpool1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.maxpool2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.maxpool3(x)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.linear1(x)</span><br><span class="line">        x = self.linear2(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">c1 = CIFAR10()</span><br><span class="line">input = torch.ones(64, 3, 32, 32)</span><br><span class="line">output = c1(input)</span><br><span class="line">print(output.shape)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">#与上面等价</span><br><span class="line">class CIFAR10(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.model1=nn.Sequential(</span><br><span class="line">            nn.Conv2d(3, 32, 5, padding=2),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Conv2d(32, 32, 5, padding=2),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Conv2d(32, 64, 5, padding=2),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(1024, 64),</span><br><span class="line">            nn.Linear(64, 10)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.model1(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">c1 = CIFAR10()</span><br><span class="line">input = torch.ones(64, 3, 32, 32)</span><br><span class="line">output = c1(input)</span><br><span class="line">writer=SummaryWriter(&quot;./logs&quot;)</span><br><span class="line">writer.add_graph(c1,input)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pPf05D0"><img src="https://z1.ax1x.com/2023/09/16/pPf05D0.png" alt="pPf05D0.png" style="zoom:25%;" /></a></p>
<h2 id="损失函数，反向传播-，优化器"><a href="#损失函数，反向传播-，优化器" class="headerlink" title="损失函数，反向传播 ，优化器"></a>损失函数，反向传播 ，优化器</h2><p>损失函数的计算：</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pPfr6UK"><img src="https://z1.ax1x.com/2023/09/16/pPfr6UK.png" alt="pPfr6UK.png" style="zoom: 50%;" /></a></p>
<p>常用的有：	</p>
<p>L1loss（），MESLoss（） 以上两个分别是 计算输入输出的差的平均值，计算输入输出方差的平均值，输入要有批次</p>
<p>交叉熵CrossEntropyLoss（），用于进行 多（x）分类 任务的损失计算</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pPfszSe"><img src="https://z1.ax1x.com/2023/09/16/pPfszSe.png" alt="pPfszSe.png" style="zoom:33%;" /></a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">inputs = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.float32)</span><br><span class="line">targets = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">inputs = torch.reshape(inputs,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">targets = torch.reshape(targets,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>) )</span><br><span class="line"></span><br><span class="line">loss = torch.nn.L1Loss()</span><br><span class="line">loss_mse = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line">result = loss(inputs, targets)</span><br><span class="line">result_mse = loss_mse(inputs, targets)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="built_in">print</span>(result_mse)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x=torch.tensor([<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>])   <span class="comment"># 赋值</span></span><br><span class="line">y=torch.tensor([<span class="number">1</span>])</span><br><span class="line">x=torch.reshape(x,(<span class="number">1</span>,<span class="number">3</span>))        <span class="comment"># 只是改变形状而已，不是赋值，一行三列</span></span><br><span class="line">cross_loss=torch.nn.CrossEntropyLoss()</span><br><span class="line"><span class="built_in">print</span>(cross_loss(x,y))</span><br></pre></td></tr></table></figure>

<p><strong>优化器的使用</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">datasets = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset2&quot;</span>, transform=torchvision.transforms.ToTensor(), train=<span class="literal">False</span>,</span><br><span class="line">                                        download=<span class="literal">False</span>)</span><br><span class="line">dataloader = DataLoader(datasets, batch_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CIFAR10</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.model1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.model1(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">c1 = CIFAR10()</span><br><span class="line">optim = torch.optim.SGD(c1.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># 优化器 随机梯度下降算法 第一个参数一般都一样  第二个参数是学习率</span></span><br><span class="line">cross_loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    running_lose = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">        imgs, tragets = data</span><br><span class="line">        output = c1(imgs)</span><br><span class="line">        loss = cross_loss(output, tragets)</span><br><span class="line">        optim.zero_grad()                       <span class="comment"># 用于清除所有优化器梯度 ,防止循环上次影响下次</span></span><br><span class="line">        loss.backward()                         <span class="comment"># 用于计算修改模型神经网络的梯度</span></span><br><span class="line">        optim.step()                            <span class="comment"># 则用于更新模型的参数</span></span><br><span class="line">        running_lose = running_lose + loss</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(running_lose)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h2 id="模型操作"><a href="#模型操作" class="headerlink" title="模型操作"></a>模型操作</h2><p>增加修改现有模型,保存模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">vgg16_true=torchvision.models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vgg16_true)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vgg16_true.classifier.add_module(<span class="string">&quot;add_linear&quot;</span>,nn.Linear)    <span class="comment"># 添加模型</span></span><br><span class="line">vgg16_true.classifier[<span class="number">6</span>]=nn.Linear(<span class="number">4096</span>,<span class="number">10</span>)     <span class="comment">#  修改模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存方式1</span></span><br><span class="line">torch.save(vgg16_true,<span class="string">&quot;vgg16true.pth&quot;</span>)      <span class="comment"># 第二个参数是路径</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存方式2（官方推荐）</span></span><br><span class="line">torch.save(vgg16_true.state_dict(),<span class="string">&quot;vgg16true2.pth&quot;</span>)    <span class="comment"># 把模型的参数保存到字典上，不保存模型</span></span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pPfgOnH"><img src="https://z1.ax1x.com/2023/09/16/pPfgOnH.png" alt="pPfgOnH.png" style="zoom:50%;" /></a></p>
<p>读取模型</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 针对保存方式1的加载(自己定义的模型，保存加载所在的python文件都需要有神经网络类的定义)</span><br><span class="line">vgg16=torch.load(&quot;vgg16true.pth&quot;)</span><br><span class="line"></span><br><span class="line"># 针对保存方式2的加载</span><br><span class="line">vgg16_2=torchvision.models.vgg16(pretrained=False)</span><br><span class="line">vgg16_2.load_state_dict(torch.load(&quot;vgg16true2.pth&quot;))</span><br></pre></td></tr></table></figure>





<h2 id="模型的训练套路"><a href="#模型的训练套路" class="headerlink" title="模型的训练套路"></a>模型的训练套路</h2><p>分类问题中准确率的计算方法：</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pPfzYCV"><img src="https://z1.ax1x.com/2023/09/17/pPfzYCV.png" alt="pPfzYCV.png" style="zoom: 33%;" /></a></p>
<p>经典的训练套路：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset4&quot;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                          download=<span class="literal">True</span>)</span><br><span class="line">test_data = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset3&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                         download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据的长度&#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(train_data)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预测数据的长度&#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(test_data)))</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_loader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CIFAR10</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.model1=nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.model1(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">C1 = CIFAR10()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">loss_fn = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">optimizer = torch.optim.SGD(C1.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练轮数</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练或者次数</span></span><br><span class="line">train_sum = <span class="number">0</span></span><br><span class="line">test_sum = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;————————————————当时正在进行第&#123;&#125;轮训练——————————————&quot;</span>.<span class="built_in">format</span>(i + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练步骤开始</span></span><br><span class="line">    C1.train()  <span class="comment"># 针对有dropout时有用，完整模板建议写上</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_loader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        output = C1(imgs)</span><br><span class="line">        loss = loss_fn(output, targets)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 优化模型</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 统计次数</span></span><br><span class="line">        train_sum = train_sum + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> train_sum % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;第&#123;&#125;次训练，loss的值为&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_sum, loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试步骤开始</span></span><br><span class="line">    C1.<span class="built_in">eval</span>()</span><br><span class="line">    total_loss_sums = <span class="number">0</span></span><br><span class="line">    total_accuary = <span class="number">0</span>  <span class="comment"># 分类任务中的整体准确率</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 此时无梯度</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            imgs, targets = data</span><br><span class="line">            output2 = C1(imgs)</span><br><span class="line">            loss = loss_fn(output2, targets)</span><br><span class="line"></span><br><span class="line">            total_loss_sums = total_loss_sums + loss</span><br><span class="line">            accuary = (output2.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line">            total_accuary = total_accuary + accuary</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的loss：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_loss_sums))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的准确率：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_accuary / <span class="built_in">len</span>(test_data)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存模型</span></span><br><span class="line">    torch.save(C1, <span class="string">&quot;C1&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型已保存&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><strong>GPU训练</strong></p>
<p>第一种方法，给以下部分加上cuda</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pPhS1MD"><img src="https://z1.ax1x.com/2023/09/17/pPhS1MD.png" alt="pPhS1MD.png" style="zoom:33%;" /></a></p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pPhStII"><img src="https://z1.ax1x.com/2023/09/17/pPhStII.png" alt="pPhStII.png"  /></a></p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pPhSdRf"><img src="https://z1.ax1x.com/2023/09/17/pPhSdRf.png" alt="pPhSdRf.png"></a></p>
<p>第二种方法，定义device</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pPhSHoR"><img src="https://z1.ax1x.com/2023/09/17/pPhSHoR.png" alt="pPhSHoR.png" style="zoom:50%;" /></a></p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pPhSqF1"><img src="https://z1.ax1x.com/2023/09/17/pPhSqF1.png" alt="pPhSqF1.png"></a></p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pPhSOW6"><img src="https://z1.ax1x.com/2023/09/17/pPhSOW6.png" alt="pPhSOW6.png"></a></p>
<h2 id="补充遇到的函数"><a href="#补充遇到的函数" class="headerlink" title="补充遇到的函数"></a>补充遇到的函数</h2><p><strong>pytorch.nn.Embedding（arg1，arg2）</strong></p>
<p>用于将离散的整数标签映射到连续的向量空间中，<code>Embedding</code>函数接受两个参数：<code>num_embeddings</code>和<code>embedding_dim</code>。<code>num_embeddings</code>表示标签的总数，即词汇表的大小；<code>embedding_dim</code>表示每个标签被映射到的向量的维度。</p>
<p><strong>torch.arange(start&#x3D;0, end, step&#x3D;1,  out&#x3D;None, dtype&#x3D;None, layout&#x3D;torch.strided, device&#x3D;None, requires_grad&#x3D;False)</strong></p>
<p>用于生成一个一维张量，其元素是连续的整数。start起始值，end结束值，例如[0,1,2,3,….end]</p>
<p><strong>torch.exp(x)</strong></p>
<p>计算指数e的x次方,结果为张量x[1,2,3]中的元素分别被求了 e 的 1 次方、e 的 2 次方、e 的 3 次方等，得到了新的张量 <code>y</code>。</p>
<p><strong>nn.Parameter(data,requires_grad)</strong></p>
<p>用于定义模型中的可学习参数,data是将要被包装的数据,这通常是一个 Tensor。requires_grad是一个布尔值，表示是否需要计算此参数的梯度(默认false)。nn.Parameter()的返回值实际上就是一个 Tensor，但这个 Tensor 有特殊的属性，即 requires_grad&#x3D;True，这表示该 Tensor 是一个可学习的参数。</p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/09/23/transformer/" rel="prev" title="transformer">
                  <i class="fa fa-chevron-left"></i> transformer
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/10/02/Windows%E6%88%96%E8%80%85Linux%E5%AE%89%E8%A3%85python%E5%8C%85%E5%AE%89%E8%A3%85%E4%B8%8D%E4%B8%8A%E9%97%AE%E9%A2%98/" rel="next" title="Windows或者Linux安装python包安装不上问题">
                  Windows或者Linux安装python包安装不上问题 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="lv-container" data-id="city" data-uid="MTAyMC81ODczNS8zNTE5Nw=="></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder"></span>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  




<script src="/js/third-party/comments/livere.js"></script>

</body>
</html>
